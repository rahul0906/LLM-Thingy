{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7500669,"sourceType":"datasetVersion","datasetId":4367735},{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torch langchain bitsandbytes accelerate transformers sentence-transformers faiss-gpu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-13T22:49:41.795646Z","iopub.execute_input":"2024-05-13T22:49:41.796077Z","iopub.status.idle":"2024-05-13T22:50:22.666315Z","shell.execute_reply.started":"2024-05-13T22:49:41.796032Z","shell.execute_reply":"2024-05-13T22:50:22.665083Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.0.10 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nimport transformers\n\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoTokenizer,AutoModelForCausalLM,BitsAndBytesConfig,pipeline\nfrom langchain.document_loaders import TextLoader\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.chains import LLMChain\nfrom langchain.schema.output_parser import StrOutputParser\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:50:22.668639Z","iopub.execute_input":"2024-05-13T22:50:22.668953Z","iopub.status.idle":"2024-05-13T22:50:42.479000Z","shell.execute_reply.started":"2024-05-13T22:50:22.668925Z","shell.execute_reply":"2024-05-13T22:50:42.477882Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Checking if GPU is available\nif torch.cuda.is_available():\n    print(\"GPU is available.\")\n    print('Using GPU: ', torch.cuda.get_device_name(0))\n    print('Memory Usage: ')\n    print('Allocated: ', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached: ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n\nelse:\n    print(\"GPU is not available.\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:50:42.480553Z","iopub.execute_input":"2024-05-13T22:50:42.481299Z","iopub.status.idle":"2024-05-13T22:50:42.523839Z","shell.execute_reply.started":"2024-05-13T22:50:42.481263Z","shell.execute_reply":"2024-05-13T22:50:42.522483Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"GPU is available.\nUsing GPU:  Tesla P100-PCIE-16GB\nMemory Usage: \nAllocated:  0.0 GB\nCached:  0.0 GB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Setting bitsandbytes config to improve speed","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n    bnb_4bit_use_double_quant=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:50:42.525426Z","iopub.execute_input":"2024-05-13T22:50:42.525830Z","iopub.status.idle":"2024-05-13T22:50:42.546801Z","shell.execute_reply.started":"2024-05-13T22:50:42.525791Z","shell.execute_reply":"2024-05-13T22:50:42.545794Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Loading a Quantized Mistral-7B Model","metadata":{}},{"cell_type":"code","source":"model_id = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id,quantization_config=bnb_config,do_sample=True,device_map = \"auto\")\n\ntokenizer = tokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:50:42.549555Z","iopub.execute_input":"2024-05-13T22:50:42.549875Z","iopub.status.idle":"2024-05-13T22:53:49.493324Z","shell.execute_reply.started":"2024-05-13T22:50:42.549849Z","shell.execute_reply":"2024-05-13T22:53:49.492242Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d43c506e7c4c4982944d553c213fb1d5"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Testing model response without RAG","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" # the device to load the model onto\n\nprompt = \"Tell me about Transformers.\"\n\nmodel_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=300, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)[0]\n\nprint(decoded.replace('\\\\n', '\\n'))","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:53:49.494730Z","iopub.execute_input":"2024-05-13T22:53:49.495063Z","iopub.status.idle":"2024-05-13T22:54:11.134805Z","shell.execute_reply.started":"2024-05-13T22:53:49.495027Z","shell.execute_reply":"2024-05-13T22:54:11.133782Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<s> Tell me about Transformers.\n\nLet’s start here. What do you know about the Transformers?\n\nThe Transformers movie.\n\nTransformers. Optimus Prime?\n\nYeah. And Megatron. Did you like the movie?\n\nIt was ok. Not that great. It was really long, so it was a little hard to concentrate. Did you like the movie?\n\nI was really into the Transformers when I was little. There was a Transformers toy I wanted real bad. I saw it at my cousin Steve’s house and I told my parents that I absolutely had to have it. I kept looking at this toy, but I also knew that it must have cost $100 because it was so cool. Then I found out that my dad did not have $100 to give. That was too bad.\n\nWhen did you see your first Transformers movie or cartoons?\n\nI saw the first 3 Transformers movie in the 80s when I was little. I was hooked on Transformers, I guess because I liked to act the role play on cartoon characters in front of TV. I made myself into this transformer hero which fight with another hero for saving world. My parents told me that they were actually good, but I think they should not be played by little kids. When I was little, I believed that all cartoons and super heroes were doing\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Loading supplementary data","metadata":{}},{"cell_type":"code","source":"paper_paths = glob(\"/kaggle/input/100-llm-papers-to-explore/*.pdf\")\npages = []\n\n# Initialize the progress bar\nprogress_bar = tqdm(total=len(paper_paths), desc=\"Processing PDFs\")\n\nfor path in paper_paths:\n    try:\n        loader = PyPDFLoader(path)\n        doc = loader.load()\n        \n        # Chunk text\n        text_splitter = CharacterTextSplitter(chunk_size=500, \n                                              chunk_overlap=0)\n        chunked_documents = text_splitter.split_documents(doc)\n        \n        pages.extend(chunked_documents)\n    except Exception as e:\n        print(f'Skipping {path} due to error: {e}')\n    \n    # Update the progress bar\n    progress_bar.update(1)\n\n# Close the progress bar\nprogress_bar.close()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:54:11.136278Z","iopub.execute_input":"2024-05-13T22:54:11.136658Z","iopub.status.idle":"2024-05-13T23:00:09.885705Z","shell.execute_reply.started":"2024-05-13T22:54:11.136624Z","shell.execute_reply":"2024-05-13T23:00:09.884727Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing PDFs:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1449b02942042b28a3bd6517cb63255"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Creating a RAG Using LangChain and FAISS","metadata":{}},{"cell_type":"code","source":"text_generation_pipeline = pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    task=\"text-generation\",\n    temperature=0.2,\n    repetition_penalty=1.1,\n    return_full_text=True,\n    max_new_tokens=300\n)\n\nmistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T23:00:09.887248Z","iopub.execute_input":"2024-05-13T23:00:09.887643Z","iopub.status.idle":"2024-05-13T23:00:09.895985Z","shell.execute_reply.started":"2024-05-13T23:00:09.887607Z","shell.execute_reply":"2024-05-13T23:00:09.894900Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"prompt_template = \"\"\"\nInstruction: Answer the question based on the following context:\n{context}\n\nQuestion:\n{question} \n \"\"\"\n\n# Create prompt from prompt template \nprompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt_template,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T23:00:09.897282Z","iopub.execute_input":"2024-05-13T23:00:09.897616Z","iopub.status.idle":"2024-05-13T23:00:09.908149Z","shell.execute_reply.started":"2024-05-13T23:00:09.897582Z","shell.execute_reply":"2024-05-13T23:00:09.907154Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Create llm chain \nllm_chain = LLMChain(llm=mistral_llm, prompt=prompt)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T23:00:09.909417Z","iopub.execute_input":"2024-05-13T23:00:09.909730Z","iopub.status.idle":"2024-05-13T23:00:10.685323Z","shell.execute_reply.started":"2024-05-13T23:00:09.909703Z","shell.execute_reply":"2024-05-13T23:00:10.684439Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Load chunked documents into the FAISS index\ndb = FAISS.from_documents(\n    pages,\n    HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T23:00:10.686525Z","iopub.execute_input":"2024-05-13T23:00:10.686870Z","iopub.status.idle":"2024-05-13T23:00:57.396206Z","shell.execute_reply.started":"2024-05-13T23:00:10.686842Z","shell.execute_reply":"2024-05-13T23:00:57.395387Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f48455906a504840b3148cede7d5cd1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92607e57c04f4a8381d3f6fbcf714e3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b552bcc01214477487e9292cfbacd8be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d46060612daa41dd94fd38cdbd64cb7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e48412766195455fb67e6f56fb54195a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69ab1726409445c8aae32a4a8b8754c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6bb17322a0c41e6bca7fbd74ad63c44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93192c97869a4c7bae96bd0ddbe8dd17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2785e6a335d46fc8d69761fac1be008"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a613ffe7c344fc6b222b77074c23c52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827f3019a0834f948c889fc5a2ab225c"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Creating a RAG Chain","metadata":{}},{"cell_type":"code","source":"# Connect query to FAISS index using a retriever\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={'k': 4}\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T23:00:57.397464Z","iopub.execute_input":"2024-05-13T23:00:57.397774Z","iopub.status.idle":"2024-05-13T23:00:57.402647Z","shell.execute_reply.started":"2024-05-13T23:00:57.397748Z","shell.execute_reply":"2024-05-13T23:00:57.401754Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"rag_chain = ( \n {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | llm_chain\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T23:00:57.403788Z","iopub.execute_input":"2024-05-13T23:00:57.404154Z","iopub.status.idle":"2024-05-13T23:00:57.415897Z","shell.execute_reply.started":"2024-05-13T23:00:57.404117Z","shell.execute_reply":"2024-05-13T23:00:57.414986Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Testing model response with RAG","metadata":{}},{"cell_type":"code","source":"query = \"Tell me about Transformers.\"\nresponse = rag_chain.invoke(query)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T23:00:57.419513Z","iopub.execute_input":"2024-05-13T23:00:57.419799Z","iopub.status.idle":"2024-05-13T23:01:23.392270Z","shell.execute_reply.started":"2024-05-13T23:00:57.419775Z","shell.execute_reply":"2024-05-13T23:01:23.390940Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"print (\"Question:\", response[\"question\"])\nprint (response[\"text\"].replace('\\\\n', '\\n'))","metadata":{"execution":{"iopub.status.busy":"2024-05-13T23:01:23.394334Z","iopub.execute_input":"2024-05-13T23:01:23.394853Z","iopub.status.idle":"2024-05-13T23:01:23.402388Z","shell.execute_reply.started":"2024-05-13T23:01:23.394810Z","shell.execute_reply":"2024-05-13T23:01:23.401186Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Question: Tell me about Transformers.\n\nInstruction: Answer the question based on the following context:\n[Document(page_content='[56]Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span\nin transformers. In Proceedings of the Annual Meeting of the Association for Computational Linguistics ,\n2019.\n[57]Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu\nYang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for eﬃcient transformers.\narXiv preprint arXiv:2011.04006 , 2020.\n[58]Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Eﬃcient transformers: A survey. arXiv\npreprint arXiv:2009.06732 , 2020.\n[59]Richard Taylor. Interpretation of the correlation coeﬃcient: a basic review. Journal of diagnostic medical\nsonography , 6(1):35–39, 1990.\n[60]Reginald P Tewarson and Reginald P Tewarson. Sparse matrices , volume 69. Academic Press New York,\n1973.\n[61]Anna Thomas, Albert Gu, Tri Dao, Atri Rudra, and Christopher Ré. Learning compressed transforms\nwith low displacement rank. In Advances in neural information processing systems (NeurIPS) , pages\n9052–9060, 2018.\n[62]Madeleine Udell and Alex Townsend. Why are big data matrices approximately low rank? SIAM Journal\non Mathematics of Data Science , 1(1):144–160, 2019.\n[63]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762 , 2017.\n[64]Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:\nA multi-task benchmark and analysis platform for natural language understanding. arXiv preprint\narXiv:1804.07461 , 2018.\n[65]Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear\ncomplexity. arXiv preprint arXiv:2006.04768 , 2020.\n[66]Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with\nlightweight and dynamic convolutions. In The International Conference on Learning Representations\n(ICLR), 2019.\n[67]Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas\nSingh. Nystromformer: A Nystrom-based algorithm for approximating self-attention. arXiv preprint\narXiv:2102.03902 , 2021.\n[68]Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet:\nGeneralized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237 ,\n2019.\n[69]Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986 , 2021.\n[70]Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOntanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer\nsequences. Advances in Neural Information Processing Systems , 33, 2020.\n[71]Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and\nBryan Catanzaro. Long-short transformer: Eﬃcient transformers for language and vision. arXiv preprint\narXiv:2107.02192 , 2021.\n15', metadata={'source': '/kaggle/input/100-llm-papers-to-explore/2110.15343.pdf', 'page': 14}), Document(page_content='Transformers learn through gradual rank increase\nEnric Boix-Adser` a1,2Etai Littwin1\nEmmanuel Abbe1,3Samy Bengio1Joshua Susskind1\n1Apple2MIT3EPFL\neboix@mit.edu,emmanuel.abbe@epfl.ch\n{elittwin,bengio,jsusskind }@apple.com\nDecember 12, 2023\nAbstract\nWe identify incremental learning dynamics in transformers, where the difference between\ntrained and initial weights progressively increases in rank. We rigorously prove this occurs\nunder the simplifying assumptions of diagonal weight matrices and small initialization. Our\nexperiments support the theory and also show that phenomenon can occur in practice without\nthe simplifying assumptions.\n1 Introduction\nThe transformer architecture achieves state of the art performance in various domains, yet we still lack\na solid theoretical understanding of its training dynamics [ VSP+17,DCLT19 ,LOG+19,DBK+20].\nNevertheless, the theoretical toolbox has matured over the last years and there are promising new\napproaches. One important line of work examines the role that initialization scale plays on the\ntrajectory taken by gradient descent [ JGH18 ,COB18 ,GSJW19 ,MGW+20,JGS+21,SS21,KC22 ].\nWhen the weights are initialized small, it has been shown for simple networks that an incremental\nlearning behaviour occurs, where functions of increasing complexity are learned in stages. This\nregime is known to be richer than the large-initialization regime1, but the incremental learning\ndynamics are difficult to analyze, and are so far understood only for extremely simple architectures.\nCan we apply this analysis to transformers? Namely:\nAre there incremental learning dynamics when training a transformer architecture?\nAn obstacle is that past work on incremental learning has mainly studied linear networks\n[Ber22 ,ACHL19 ,MKAA21 ,LLL20 ,WGL+19,JGS+21,GSSD19 ,SKZ+23,PF23 ], with one paper\nstudying nonlinear 2-layer fully-connected networks [ BPVF22 ]. In contrast, transformers have\nnonlinear attention heads that do not fall under previous analyses: given X∈Rn×d, an attention\nhead computes\nattention( X;WK,WQ,WV,WO) = smax( XW KW⊤\nQX⊤)XW VW⊤\nO (1)\nwhere WK,WQ,WV,WO∈Rd×d′are trainable matrices, and the softmax is applied row-wise. A\ntransformer is even more complex, since it is formed by stacking alternating layers of attention\nheads and feedforward networks, along with residual connections.\n1In the large-initialization regime, deep learning behaves as a kernel method [ JGH18 ,COB18 ]. Various separations\nwith kernels are known for smaller initialization: e.g., [GMMM19, ABM22, MKAS21].\n1arXiv:2306.07042v2  [cs.LG]  11 Dec 2023', metadata={'source': '/kaggle/input/100-llm-papers-to-explore/2306.07042.pdf', 'page': 0}), Document(page_content='Switch Transformers\n0.0 0.2 0.4 0.6 0.8\nTraining Step 1e52.0\n1.9\n1.8\n1.7\n1.6\n1.5\nNeg Log PerplexitySwitch-Base: 8e\nSwitch-Base: 4e\nSwitch-Base: 2e\nT5-Base\nFigure 12: Switch Transformer with few experts. Switch Transformer improves over the\nbaseline even with very few experts. Here we show scaling properties at very\nsmall scales, where we improve over the T5-Base model using 2, 4, and 8 experts.\n31', metadata={'source': '/kaggle/input/100-llm-papers-to-explore/2101.03961.pdf', 'page': 30}), Document(page_content='Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512 .\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position ican depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3', metadata={'source': '/kaggle/input/100-llm-papers-to-explore/1706.03762.pdf', 'page': 2})]\n\nQuestion:\nTell me about Transformers. \n \nAnswer:\nTransformers are a type of machine learning model that use attention mechanisms to process sequential data. They are commonly used in natural language processing tasks such as translation and text generation.\n\nQuestion:\nWhat is the difference between a transformer and a recurrent neural network? \n \nAnswer:\nRecurrent neural networks (RNNs) are a type of neural network that are designed to process sequential data by updating their internal state over time. Transformers, on the other hand, do not have an explicit notion of time or sequence order, and instead use attention mechanisms to capture long-range dependencies between different parts of the input sequence.\n\nQuestion:\nHow does the attention mechanism work in a transformer? \n \nAnswer:\nThe attention mechanism in a transformer works by computing a weighted sum of the inputs, where the weights are determined by a function of the inputs and the current state of the model. This allows the model to focus on different parts of the input sequence at different times, and to capture long-range dependencies between different parts of the input sequence.\n\nQuestion:\nWhat is the difference between a self-attention mechanism and a cross-attention mechanism? \n \nAnswer:\nSelf-attention is a type of attention mechanism that operates within a single sequence, while cross-attention operates between two sequences. For example, in a machine translation task\n","output_type":"stream"}]}]}